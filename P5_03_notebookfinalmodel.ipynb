{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "searching-enlargement",
   "metadata": {},
   "source": [
    "# Projet 5 : Catégorisez automatiquement des questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-diesel",
   "metadata": {},
   "source": [
    "L'objectif de ce projet est d'appliquer des tags à des questions issues du site stackoverflow. Pour cela on va utiliser l'outil StackExchange qui permet de lancer des requêtes SQL sur la base de données de stackoverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-cartoon",
   "metadata": {},
   "source": [
    "## Notebook 3 : Modèle final\n",
    "* Chargement des données\n",
    "* Split des données\n",
    "* Preprocess du texte\n",
    "* Entrainement du modèle final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-subscriber",
   "metadata": {},
   "source": [
    "## Librairies  utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "municipal-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# text preprocessing\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nltk.download()\n",
    "n_jobs = -1 # enable multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-essence",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adapted-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_process.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tags_process'] = df['Tags_process'].apply(lambda x: [text[1:-1] for text in x.strip('[]').split(', ')])\n",
    "df['Tags'] = df['Tags'].apply(lambda x: [text[1:-1] for text in x.strip('[]').split(', ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-willow",
   "metadata": {},
   "source": [
    "## Split des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "italic-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "def iterative_train_test_split(X, y, train_size):\n",
    "    \"\"\"Custom iterative train test split which\n",
    "    'maintains balanced representation with respect\n",
    "    to order-th label combinations.'\n",
    "    \"\"\"\n",
    "    stratifier = IterativeStratification(\n",
    "        n_splits=2, order=1, sample_distribution_per_fold=[1.0-train_size, train_size, ])\n",
    "    train_indices, test_indices = next(stratifier.split(X, y))\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "featured-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Body'].values\n",
    "y = df['Tags_process'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "configured-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_mlb = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chubby-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = iterative_train_test_split(X, y_mlb, train_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-orchestra",
   "metadata": {},
   "source": [
    "## Preprocess du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedicated-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = nltk.corpus.stopwords.words('english')\n",
    "sw.extend(['error', 'code', 'program', 'question', 'result'])\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thorough-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTag(text):\n",
    "    tag_list = ['code','a','img','kbd','del','strike','s']\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    for tag in tag_list:\n",
    "        for tagless in soup.find_all(tag):\n",
    "            tagless.decompose()\n",
    "            \n",
    "    # to get lowercase text\n",
    "    return soup.get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "planned-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    cleaned = re.sub('\\n',r' ',text)\n",
    "    # It is prefereable to replace punctuation char by white space to avoid creating new words\n",
    "    translate_table = dict((ord(char), ' ') for char in punctuation)   \n",
    "    cleaned = cleaned.translate(translate_table)\n",
    "    cleaned = re.sub(r'\\s+', ' ',cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "micro-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreprocessingString(text, allowed_postags=['NOUN']):\n",
    "    doc = nlp(text)\n",
    "    cleaned = \" \".join([token.lemma_ for token in doc if ((token.pos_ in allowed_postags) and (token.text not in sw))])\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "looking-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreprocessing(text):\n",
    "    text_notag = removeTag(text)\n",
    "    text_nopunct = removePunctuation(text_notag)\n",
    "    \n",
    "    return textPreprocessingString(text_nopunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-anthropology",
   "metadata": {},
   "source": [
    "## Entrainement du modèle final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "portable-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_best = Pipeline(steps=[('transformer', TfidfVectorizer(lowercase=False, preprocessor=textPreprocessing, max_df=0.11, min_df=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bronze-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gb = GradientBoostingClassifier(random_state=0,\n",
    "                                     max_depth=5,\n",
    "                                     max_features='auto',\n",
    "                                     min_samples_leaf=5,\n",
    "                                     min_samples_split=2,\n",
    "                                     n_estimators=150\n",
    "                                    )\n",
    "ovr_gb = OneVsRestClassifier(base_gb, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ethical-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = Pipeline(steps=[('preprocessor', preprocessor_best),('model', ovr_gb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "naughty-surname",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 Pipeline(steps=[('transformer',\n",
       "                                  TfidfVectorizer(lowercase=False, max_df=0.11,\n",
       "                                                  min_df=0,\n",
       "                                                  preprocessor=<function textPreprocessing at 0x7f18742555e0>))])),\n",
       "                ('model',\n",
       "                 OneVsRestClassifier(estimator=GradientBoostingClassifier(max_depth=5,\n",
       "                                                                          max_features='auto',\n",
       "                                                                          min_samples_leaf=5,\n",
       "                                                                          n_estimators=150,\n",
       "                                                                          random_state=0),\n",
       "                                     n_jobs=-1))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-senate",
   "metadata": {},
   "source": [
    "### Score train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "front-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.8893137401399684\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_pipeline.predict(X_train)\n",
    "score = jaccard_score(y_train, y_pred, average='macro')\n",
    "print(f'score : {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-residence",
   "metadata": {},
   "source": [
    "### Score test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "changing-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.2618721122631574\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_pipeline.predict(X_test)\n",
    "score = jaccard_score(y_test, y_pred, average='macro')\n",
    "print(f'score : {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-guide",
   "metadata": {},
   "source": [
    "## Sauvegarde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-speech",
   "metadata": {},
   "source": [
    "### Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "casual-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the multilabel binarizer with Pickle\n",
    "mlb_pkl_filename = 'mlb.pkl'\n",
    "# Open the file to save as pkl file\n",
    "mlb_pkl = open(mlb_pkl_filename, 'wb')\n",
    "pickle.dump(mlb, mlb_pkl)\n",
    "# Close the pickle instances\n",
    "mlb_pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-nothing",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pursuant-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the pipeline with Pickle\n",
    "best_pipeline_pkl_filename = 'best_pipeline.pkl'\n",
    "# Open the file to save as pkl file\n",
    "best_pipeline_pkl = open(best_pipeline_pkl_filename, 'wb')\n",
    "pickle.dump(best_pipeline, best_pipeline_pkl)\n",
    "# Close the pickle instances\n",
    "best_pipeline_pkl.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
